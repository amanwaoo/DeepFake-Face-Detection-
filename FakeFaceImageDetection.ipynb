{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnMNcrwOJ5Eh","executionInfo":{"status":"ok","timestamp":1679698774785,"user_tz":240,"elapsed":21171,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"673ae94e-7c91-4506-8232-14ea403c4ef0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","Folder exists\n"]}],"source":["from pathlib import Path\n","USE_COLAB: bool = True\n","dataset_base_path = Path(\"/content/drive/My Drive/ECE 792 - Advance Topics in Machine Learning/Datasets\")\n","if USE_COLAB:\n","  from google.colab import drive\n","  \n","  # Mount the drive to access google shared docs\n","  drive.mount('/content/drive/', force_remount=True)\n","\n","  if dataset_base_path.exists():\n","    print(\"Folder exists\")\n","  else:\n","    print(\"DOESN'T EXIST. Add desired folder as a shortcut in your 'My Drive'\")"]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim as optim\n","import torch.utils.data\n","import torchvision.datasets as dataset\n","import torchvision.transforms as transforms\n","import torchvision.utils as vutils\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from IPython.display import HTML\n","from typing import Tuple, Optional, List\n","\n","import argparse\n","import os\n","from tqdm import tqdm\n","import time\n","import copy\n","import math\n","from zipfile import ZipFile\n","\n","from PIL import Image\n","from typing import Dict, List, Union\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# PyTorch's versions:\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)\n","print(\"NumPy Version: \",np.__version__)\n","gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3BAhDaXMYfd","executionInfo":{"status":"ok","timestamp":1680798482507,"user_tz":240,"elapsed":9638,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"adee2f59-41d8-4654-8b85-f7f941360516"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch Version:  2.0.0+cu118\n","Torchvision Version:  0.15.1+cu118\n","NumPy Version:  1.22.4\n","Thu Apr  6 16:28:03 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["import random\n","\n","class CelebrityData(torch.utils.data.Dataset):\n","\n","  def __init__(\n","    self,\n","    base_path: Path,\n","    transform = None,\n","    seed = None,\n","    rng = None,\n","    gans_to_skip: Optional[List[str]] = None,\n","    n_fake_imgs_to_extract: int = 40000,\n","    validation_ratio: float = 0.0,\n","    *,\n","    fake_imgs: Optional[Dict[str, List[Union[str, Path]]]] = None,  # if unzip_fake_imgs is True, fake_imgs is ignored\n","    real_imgs: Optional[List[str]] = None,  # if unzip_real_imgs is True, real_imgs is ignored\n","    unzip_fake_imgs: bool = True,\n","    unzip_real_imgs: bool = True,\n","    # below keywords are dummies used to pass a CelebrityData instance to CelebrityDataCFFN & CelebrityDataClassificationNetwork for validation & training set use case\n","    already_shuffled = None,\n","    fake_image_gan_names = None,\n","    fake_images_path = None,\n","    n_fake_gans = None,\n","    real_imgs_zip_file = None,\n","    ):\n","    '''\n","    Folder structure of base_path should be\n","    base_path -> RealFaces/FakeFaces\n","    RealFaces -> .zip\n","    FakeFaces -> GANType -> .zip (e.g., FakeFaces -> PGGAN -> .zip)\n","    '''\n","    super().__init__()\n","    if rng is not None:\n","      self.rng = rng\n","    else:\n","      self.rng = np.random.default_rng(seed)\n","    self.unzip_real_imgs = unzip_real_imgs\n","    self.unzip_fake_imgs = unzip_fake_imgs\n","    self.validation_ratio = validation_ratio\n","    self.already_shuffled = False\n","    self.base_path = base_path\n","\n","    if self.unzip_fake_imgs:\n","      self.fake_images_path = Path(base_path) / \"FakeFaces\"\n","      self.fake_image_gan_names = os.listdir(str(self.fake_images_path))\n","      if gans_to_skip is not None:\n","        print(f\"Not unzipping '{gans_to_skip}'\")\n","        self.fake_image_gan_names = list(filter(lambda x: x not in gans_to_skip, self.fake_image_gan_names))\n","      self.fake_imgs: Dict[str, List[Union[str, Path]]] = {}\n","      for fake_image_gan_name in self.fake_image_gan_names:\n","        print(f\"Extracting imagery for '{fake_image_gan_name}'\")\n","        fake_image_gan_path = self.fake_images_path / fake_image_gan_name\n","        zip_file = sorted(fake_image_gan_path.glob(\"*.zip\"))\n","        if len(zip_file) == 1:\n","          zip_file = zip_file[0]\n","          with ZipFile(str(zip_file), 'r') as zipObj:\n","            zipObj.extractall()\n","          self.fake_imgs.update({fake_image_gan_name: zipObj.namelist()[:n_fake_imgs_to_extract]})\n","        else:\n","          fake_image_paths = sorted(fake_image_gan_path.glob(\"*.jpg\"))\n","          self.fake_imgs.update({fake_image_gan_name: fake_image_paths[:n_fake_imgs_to_extract]})\n","    elif fake_imgs is not None:\n","      self.fake_imgs = fake_imgs\n","      self.fake_image_gan_names = list(self.fake_imgs.keys())\n","      self.fake_images_path = None\n","    else:\n","      self.fake_imgs = {}\n","      self.fake_image_gan_names = []\n","      self.fake_images_path = None\n","\n","    self.n_fake_gans = len(list(self.fake_imgs.keys()))\n","\n","    if unzip_real_imgs:\n","      self.real_images_path = Path(base_path) / \"RealFaces\"\n","      print(\"Extracting RealFaces imagery\")\n","      real_images_zip_files = sorted(self.real_images_path.glob(\"*.zip\"))\n","      if len(real_images_zip_files) != 1:\n","        raise RuntimeError(f\"Got more than or less than 1 zip file in '{self.real_images_path}'. Got '{len(real_images_zip_files)}'\")\n","      self.real_images_zip_file = real_images_zip_files[0]\n","      # Create a ZipFile Object and load sample.zip in it\n","      with ZipFile(str(self.real_images_zip_file), 'r') as zipObj:\n","        # Extract all the contents of zip file in current directory\n","        zipObj.extractall()\n","      if self.len_of_fake_imgs >= len(zipObj.namelist()):\n","        self.real_imgs = zipObj.namelist()[1:]\n","      else:\n","        self.real_imgs = zipObj.namelist()[1:self.len_of_fake_imgs+1]\n","    elif real_imgs is not None:\n","      self.real_imgs = real_imgs\n","      self.real_imgs_zip_file = None\n","    else:\n","      self.real_imgs = []\n","      self.real_imgs_zip_file = None\n","\n","    self.transform = transform\n","\n","    # # according to Deep Fake Image Detection Based on Pairwise Learning, we need to make combinations for all\n","    # # real images with all fake images\n","    # fake_img_list = []\n","    # for fake_imgs in self.fake_imgs.values():\n","    #   fake_img_list.extend(fake_imgs)\n","    # self.fake_img_list = fake_img_list\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=len(self.fake_imgs) - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","    \n","    return self.fake_imgs.get(gan_selection)[index // len(self.fake_imgs)]\n","\n","  @property\n","  def len_of_fake_imgs(self) -> int:\n","    total_len = 0\n","    for val in self.fake_imgs.values():\n","      total_len += len(val)\n","\n","    return total_len\n","\n","  def len_of_real_and_fake(self):\n","    return len(self.real_imgs) + self.len_of_fake_imgs\n","\n","  def get_validation_set(self, shuffle: bool = True):\n","    if self.validation_ratio > 0:\n","      fake_imgs = {}\n","      fake_imgs_new = {}\n","      for key, val in self.fake_imgs.items():\n","        val_len = len(val)\n","        validation_beg = int((1 - self.validation_ratio) * val_len)\n","        if shuffle and not self.already_shuffled:\n","          self.rng.shuffle(val)\n","        validation_vals = val[validation_beg:]\n","        fake_imgs.update({key: validation_vals})\n","        fake_imgs_new.update({key: val})\n","      validation_beg = int((1 - self.validation_ratio) * len(self.real_imgs))\n","      if shuffle and not self.already_shuffled:\n","        self.rng.shuffle(self.real_imgs)\n","      real_imgs = self.real_imgs[validation_beg:]\n","\n","      self.fake_imgs = fake_imgs_new\n","      if shuffle:\n","        self.already_shuffled = True\n","      \n","      return CelebrityData(\n","          base_path=None,\n","          transform=self.transform,\n","          rng=self.rng,\n","          validation_ratio=0.0,\n","          fake_imgs=fake_imgs,\n","          real_imgs=real_imgs,\n","          unzip_fake_imgs=False,\n","          unzip_real_imgs=False,\n","      )\n","    else:\n","      print(f\"Not returning validation Dataset as validation_ratio == 0.\")\n","      return None\n","\n","  def get_training_set(self, shuffle: bool = True):\n","    if self.validation_ratio > 0:\n","      fake_imgs = {}\n","      fake_imgs_new = {}\n","      for key, val in self.fake_imgs.items():\n","        val_len = len(val)\n","        training_end = int((1 - self.validation_ratio) * val_len)\n","        if shuffle and not self.already_shuffled:\n","          self.rng.shuffle(val)\n","        validation_vals = val[:training_end]\n","        fake_imgs.update({key: validation_vals})\n","        fake_imgs_new.update({key: val})\n","      training_end = int((1 - self.validation_ratio) * len(self.real_imgs))\n","      if shuffle and not self.already_shuffled:\n","        self.rng.shuffle(self.real_imgs)\n","      real_imgs = self.real_imgs[:training_end]\n","      \n","      self.fake_imgs = fake_imgs_new\n","\n","      if shuffle:\n","        self.already_shuffled = True\n","    \n","      return CelebrityData(\n","          base_path=None,\n","          transform=self.transform,\n","          rng=self.rng,\n","          validation_ratio=0.0,\n","          fake_imgs=fake_imgs,\n","          real_imgs=real_imgs,\n","          unzip_fake_imgs=False,\n","          unzip_real_imgs=False,\n","      )\n","    else:\n","      print(f\"Not returning training Dataset as validation_ratio == 0.\")\n","      return None"],"metadata":{"id":"feIzG8dArDEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from dataclasses import dataclass\n","from typing import List\n","\n","@dataclass\n","class Loss:\n","  loss_vals_per_batch: List[float]\n","  loss_vals_per_epoch: List[float]\n","  batch_cnt: int = 0\n","  previous_batch_cnt: int = 0\n","  epoch_cnt: int = 0\n","\n","  @classmethod\n","  def init(cls) -> \"Loss\":\n","    return cls(\n","        loss_vals_per_batch=[],\n","        loss_vals_per_epoch=[],\n","        batch_cnt=0,\n","        previous_batch_cnt=0,\n","        epoch_cnt=0,\n","    )\n","\n","  def __add__(self, other: Union[float, int]) -> \"Loss\":\n","    self.loss_vals_per_batch.append(other)\n","    self.batch_cnt += 1\n","    return self\n","\n","  def __iadd__(self, other: Union[float, int]) -> \"Loss\":\n","    return self.__add__(other)\n","\n","  @property\n","  def current_loss(self) -> float:\n","    return np.sum(self.loss_vals_per_batch) / self.batch_cnt\n","\n","  @property\n","  def previous_loss(self) -> float:\n","    if len(self.loss_vals_per_batch) > 1:\n","      return np.sum(self.loss_vals_per_batch[:-2]) / (self.batch_cnt - 1)\n","    else:\n","      return 0\n","\n","  def update_for_epoch(self):\n","    self.epoch_cnt += 1\n","    self.loss_vals_per_epoch.append(\n","        sum(self.loss_vals_per_batch[self.previous_batch_cnt:self.batch_cnt])\n","        / (self.batch_cnt - self.previous_batch_cnt)\n","    )\n","    self.previous_batch_cnt = self.batch_cnt"],"metadata":{"id":"Rgg-lMWkxUbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from typing import Optional\n","\n","def plot_accuracy_or_loss(\n","  train_vals: List[float],\n","  output_path: Union[str, Path],\n","  validation_vals: Optional[List[float]] = None,\n","  test_vals: Optional[List[float]] = None,\n","  title: Optional[str] = None,\n","  ylabel: Optional[str] = None,\n","  xlabel: Optional[str] = None,\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  if plot_labels is None:\n","    plot_labels = [\"train\"]\n","    if validation_vals is not None:\n","      plot_labels.append(\"validation\")\n","    if test_vals is not None:\n","      plot_labels.append(\"test\")\n","  elif not isinstance(plot_labels, list):\n","    plot_labels = [plot_labels] * 3\n","\n","  x_epochs = np.arange(1, len(train_vals) + 1)\n","  plt.plot(x_epochs, train_vals, label=plot_labels[0])\n","  if validation_vals is not None:\n","    x_epochs = np.arange(len(train_vals) - len(validation_vals) + 1, len(train_vals) + 1)\n","    plt.plot(x_epochs, validation_vals, label=plot_labels[1])\n","  if test_vals is not None:\n","    x_epochs = np.arange(len(train_vals) - len(test_vals) + 1, len(train_vals) + 1)\n","    plt.plot(x_epochs, test_vals, label=plot_labels[2])\n","  if title is not None:\n","    plt.title(title)\n","  if ylabel is not None:\n","    plt.ylabel(ylabel)\n","  if xlabel is not None:\n","    plt.xlabel(xlabel)\n","  plt.legend()\n","  plt.savefig(output_path)\n","  plt.close()\n","\n","\n","def save_loss_plot(\n","  train_loss: List[float],\n","  output_path: Union[str, Path],\n","  test_loss: Optional[List[float]] = None,\n","  val_loss: Optional[List[float]] = None,\n","  title: str = \"Loss\",\n","  ylabel: str = \"Loss\",\n","  xlabel: str = \"Epochs\",\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  plot_accuracy_or_loss(\n","    train_vals=train_loss,\n","    output_path=output_path,\n","    validation_vals=val_loss,\n","    test_vals=test_loss,\n","    title=title,\n","    ylabel=ylabel,\n","    xlabel=xlabel,\n","    plot_labels=plot_labels,\n","  )\n","\n","def save_accuracy_plot(\n","  train_acc: List[float],\n","  output_path: Union[str, Path],\n","  test_acc: Optional[List[float]] = None,\n","  val_acc: Optional[List[float]] = None,\n","  plot_labels: Optional[Union[str, List[str]]] = None,\n","):\n","  plot_accuracy_or_loss(\n","    train_vals=train_acc,\n","    output_path=output_path,\n","    validation_vals=val_acc,\n","    test_vals=test_acc,\n","    title=\"Accuracy\",\n","    ylabel=\"Accuracy\",\n","    xlabel=\"Epochs\",\n","    plot_labels=plot_labels,\n","  )"],"metadata":{"id":"oAqcebVRywER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","from tqdm import tqdm\n","\n","def get_latest_model(base_path, suffix: str = \".pth\") -> Path:\n","  epoch_num = []\n","  # all_files = sorted(Path(base_path).glob(suffix))\n","  all_files = os.listdir(str(base_path))\n","  for file_ in tqdm(all_files):\n","    idx_num = re.search(\"--\", str(file_)).span()\n","    idx_pt = re.search(suffix, str(file_)).span()\n","    model_num = str(file_)[idx_num[-1]:idx_pt[0]]\n","    try:\n","      epoch_num.append(int(model_num))\n","    except ValueError:\n","      idx_num = re.search(\"--\", str(model_num)).span()\n","      epoch_num.append(int(model_num[idx_num[-1]:]))\n","\n","  idx = epoch_num.index(np.max(epoch_num))\n","  return Path(base_path) / all_files[idx]"],"metadata":{"id":"sM86rgUEC9tf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","def number_of_combinations(n_objs: int, r_at_a_time: int) -> int:\n","  num = math.factorial(n_objs)\n","  den = math.factorial(n_objs - r_at_a_time) * math.factorial(r_at_a_time)\n","  return int(num / den)\n","\n","def get_n_objs_for_a_number_of_combinations_with_2_at_a_time(combs: int) -> int:\n","  return int((1 + math.sqrt(1 + (4*combs * 2))) / 2)"],"metadata":{"id":"t7kOC23hzLu6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Two-step learning policy as employed by 'Deep Fake Image Detection Based on Pairwise Learning'. Therefore, we first train the CFFN network with the contrastive loss. After the CFFN network is learned to minimize the contrastive loss, we then train the classification network using the outputs from the CFFN network, as this better feature representation output will allow the classification network to better classify the images as fake or real. The classification network is trained using the binary cross-entropy loss of predicting whether the image is real or fake [p.6. section 2.4]"],"metadata":{"id":"QLtX8bv5Qw_6"}},{"cell_type":"markdown","source":["**1. Common Fake Feature Network**\n","\n","Network structure includes a pairwise learning approach. \"A fake face image detector based on the novel CFFN, consisting of an improved DenseNet backbone network and Siamese network architecture...The cross-layer features are investigated by the proposed CFFN, which can be used to improve the performance.\"\n","\n","The fake and real images are paired together and the pairwise information is used to construct the contrastive loss to learn the discriminative common fake feature (CFF) by the CFFN. The paper states that 2 million pairwise samples are used for training.\n","\n","\"One way to learn both the CFFs and classifier is the join learning strategy incorporating the contrastive loss and cross-entropy loss into the total energy function. In another way, the CFFN is first trained by the proposed contrastive loss and follows by training the classifier based on cross-entropy loss. When the first strategy is applied, it is difficult to observe the impact of both contrastive and cross-entropy loss functions on the performance of the fake image detection tasks. Therefore, we adopt the second strategy to ensure the best performance of the proposed method.\""],"metadata":{"id":"RkqQitJIUMJf"}},{"cell_type":"code","source":["import itertools\n","from typing import Tuple\n","class CelebrityDataCFFN(CelebrityData):\n","\n","  def __init__(\n","      self,\n","      base_path: Path,\n","      transform = None,\n","      seed = None,\n","      n_combinations: int = 4e6,\n","      gans_to_skip: Optional[List[str]] = None,\n","      *,\n","      celebrity_data: Optional[CelebrityData] = None,\n","    ):\n","    if celebrity_data is None:\n","      super().__init__(base_path=base_path, transform=transform, seed=seed, gans_to_skip=gans_to_skip)\n","    else:\n","      super().__init__(**celebrity_data.__dict__)\n","\n","    # for training the CFFN we want fake-fake pairs & real-real pairs\n","    # self.n_fake_combinations = self.n_fake_gans * number_of_combinations(int(self.len_of_fake_imgs / self.n_fake_gans), 2)\n","    self.n_imgs_for_combinations = get_n_objs_for_a_number_of_combinations_with_2_at_a_time(n_combinations)\n","    # only making combinations between images made by the same GAN\n","    # could try experimenting with combinations of images between different GANs\n","    self.fake_image_combos: Dict[str, list] = {}\n","    for gan_name, img_list in self.fake_imgs.items():\n","      self.fake_image_combos.update({gan_name: list(itertools.combinations(img_list[:self.n_imgs_for_combinations], 2))})\n","    \n","    self.real_image_combos = list(itertools.combinations(self.real_imgs[:self.n_imgs_for_combinations], 2))\n","\n","  def __getitem__(self, index):\n","    img0_path, img1_path, pair_indicator = self.choose_real_or_fake_pair(index)\n","    img0 = Image.open(img0_path).convert('RGB')\n","    if self.transform is not None:\n","      img0 = self.transform(img0)\n","\n","    img1 = Image.open(img1_path).convert('RGB')\n","    if self.transform is not None:\n","      img1 = self.transform(img1)\n","\n","    return img0, img1, pair_indicator\n","\n","  # def choose_real_or_fake_pair(self, index) -> Tuple[str, str, int]:\n","  #   if self.rng.standard_normal() > 0:\n","  #     img_pair = next(itertools.islice(self.real_image_combos, index, None))\n","  #     pair_indicator = 1\n","  #   else:\n","  #     img_pair = self.fake_image_rand_selection(index)\n","  #     pair_indicator = 0\n","\n","  #   return img_pair[0], img_pair[1], pair_indicator\n","\n","  def choose_real_or_fake_pair(self, index) -> Tuple[str, str, int]:\n","    if self.rng.standard_normal() > 0:\n","      img_pair = self.real_image_combos[index]\n","      pair_indicator = 1\n","    else:\n","      img_pair = self.fake_image_rand_selection(index)\n","      pair_indicator = 0\n","\n","    return img_pair[0], img_pair[1], pair_indicator\n","\n","  # def fake_image_rand_selection(self, index) -> Tuple[str, str]:\n","  #   rand_selection = self.rng.uniform(low=-0.499, high=len(self.fake_imgs) - 0.501)\n","  #   gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","  #   iter_combo = self.fake_image_combos.get(gan_selection)\n","  #   return next(itertools.islice(iter_combo, index // len(self.fake_image_gan_names), None))\n","\n","  def fake_image_rand_selection(self, index) -> Tuple[str, str]:\n","    rand_selection = self.rng.uniform(low=-0.499, high=self.n_fake_gans - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","\n","    return self.fake_image_combos[gan_selection][index]\n","\n","  def __len__(self):\n","    return len(self.real_image_combos)\n","\n","  @classmethod\n","  def get_training_and_validation_set(\n","    cls,\n","    base_path: Path,\n","    transform = None,\n","    seed = None,\n","    gans_to_skip: Optional[List[str]] = None,\n","    n_fake_imgs_to_extract: int = 40000,\n","    validation_ratio: float = 0.0,\n","    n_combinations_train: int = 4e6,\n","    n_combinations_val: int = 1e4,\n","    shuffle: bool = True,\n","  ):\n","    celebrity_data = CelebrityData(\n","      base_path=base_path,\n","      transform=transform,\n","      seed=seed,\n","      gans_to_skip=gans_to_skip,\n","      n_fake_imgs_to_extract=n_fake_imgs_to_extract,\n","      validation_ratio=validation_ratio,\n","    )\n","    val_set = celebrity_data.get_validation_set(shuffle)\n","    train_set = celebrity_data.get_training_set(shuffle)\n","\n","    return cls(\n","      base_path=base_path,\n","      n_combinations=n_combinations_val,\n","      celebrity_data=val_set,\n","    ), cls(\n","      base_path=base_path,\n","      n_combinations=n_combinations_train,\n","      celebrity_data=train_set\n","    )"],"metadata":{"id":"BdmG6jjDMhXK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We will be working with GPU:\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print('Device : ' , device)\n","\n","# Number of GPUs available. \n","num_GPU = torch.cuda.device_count()\n","print('Number of GPU : ', num_GPU)\n","\n","# model_output_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model\")\n","# if not model_output_path.exists():\n","  # model_output_path.mkdir(exist_ok=True, parents=True)\n","\n","config_cffn = { 'batch_size'             : 88,\n","                'image_size'             : 64,\n","                'n_channel'              : 3,\n","                'n_epochs'               : 15,\n","                'lr'                     : 1e-3,\n","                'growth_rate'            : 24,\n","                'transition_layer_theta' : 0.5,\n","                'device'                 : device,\n","                'm_th'                   : 0.5,\n","                'n_combinations'         : 2e6,\n","                'seed'                   : 999,\n","                # 'model_output_path'      : model_output_path,\n","                'chkp_freq'              : 1,  # number of epochs to save model out\n","                'n_workers'              : 4,\n","                # 'gans_to_skip'           : [\"CDCGAN\", \"DCGAN\", \"WGAN-CP\", \"WGAN-GP\", \"PGGAN\"],\n","                'val_ratio'              : 0.1,\n","}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QA-ybvdX_aVl","executionInfo":{"status":"ok","timestamp":1680798549917,"user_tz":240,"elapsed":30,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"704dd2cf-cf0b-4afe-92e1-bd27b20d75f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Device :  cuda\n","Number of GPU :  1\n"]}]},{"cell_type":"code","source":["celebrity_data_val_cffn, celebrity_data_train_cffn = CelebrityDataCFFN.get_training_and_validation_set(\n","  base_path=dataset_base_path,\n","  transform=transforms.Compose(\n","    [\n","      transforms.Resize(int(config_cffn[\"image_size\"] * 1.1)),\n","      transforms.CenterCrop(config_cffn[\"image_size\"]),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","  ),\n","  seed=config_cffn.get(\"seed\"),\n","  gans_to_skip=config_cffn.get(\"gans_to_skip\"),\n","  validation_ratio=config_cffn[\"val_ratio\"],\n","  n_combinations_train=config_cffn[\"n_combinations\"],\n","  n_combinations_val=config_cffn[\"n_combinations\"] * config_cffn[\"val_ratio\"],\n",")\n","\n","dataloader_val_cffn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_val_cffn,\n","  shuffle=True,\n","  batch_size=config_cffn[\"batch_size\"],\n","  num_workers=config_cffn[\"n_workers\"],\n","  drop_last=True,  # drop last batch that may not be the same size as the expected batch for the network\n","  pin_memory=True,\n",")\n","dataloader_train_cffn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_train_cffn,\n","  shuffle=True,\n","  batch_size=config_cffn[\"batch_size\"],\n","  num_workers=config_cffn[\"n_workers\"],\n","  drop_last=True,\n","  pin_memory=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HxGy5KOVi0Id","executionInfo":{"status":"ok","timestamp":1679630121138,"user_tz":240,"elapsed":65496,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"378c3d45-33c0-4e13-f1e2-6bd26040f86a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting imagery for 'WGAN-CP'\n","Extracting imagery for 'PGGAN'\n","Extracting imagery for 'DCGAN'\n","Extracting imagery for 'LSGAN'\n","Extracting imagery for 'CDCGAN'\n","Extracting imagery for 'WGAN-GP'\n","Extracting RealFaces imagery\n"]}]},{"cell_type":"code","source":["from typing import Callable, Tuple\n","\n","class DenseBlock2(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation1_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock3(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation2_out)\n","\n","    return self.trans_layer_out\n","\n","class DenseBlock4(nn.Module):\n","  conv0_0_out = None\n","  conv0_1_out = None\n","  batch_norm0_out = None\n","  concat0_out = None\n","  activation0_out = None\n","  conv1_0_out = None\n","  conv1_1_out = None\n","  batch_norm1_out = None\n","  concat1_out = None\n","  activation1_out = None\n","  conv2_0_out = None\n","  conv2_1_out = None\n","  batch_norm2_out = None\n","  concat2_out = None\n","  activation2_out = None\n","  conv3_0_out = None\n","  conv3_1_out = None\n","  batch_norm3_out = None\n","  concat3_out = None\n","  activation3_out = None\n","  trans_layer_out = None\n","  def __init__(\n","    self,\n","    in_channels: int,\n","    out_channels: int,\n","    growth_rate: int,\n","    transition_layer_theta: float,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    self.conv0_0 = nn.Conv2d(\n","      in_channels=in_channels,\n","      out_channels=in_channels * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv0_1 = nn.Conv2d(\n","      in_channels=in_channels * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm0 = nn.BatchNorm2d(in_channels + growth_rate, device=device)\n","\n","    self.conv1_0 = nn.Conv2d(\n","      in_channels=in_channels + growth_rate,\n","      out_channels=(in_channels + growth_rate) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv1_1 = nn.Conv2d(\n","      in_channels=(in_channels + growth_rate) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm1 = nn.BatchNorm2d(in_channels + (2 * growth_rate), device=device)\n","\n","    self.conv2_0 = nn.Conv2d(\n","      in_channels=in_channels + (2 * growth_rate),\n","      out_channels=(in_channels + (2 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv2_1 = nn.Conv2d(\n","      in_channels=(in_channels + (2 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm2 = nn.BatchNorm2d((in_channels + (3 * growth_rate)), device=device)\n","\n","    self.conv3_0 = nn.Conv2d(\n","      in_channels=in_channels + (3 * growth_rate),\n","      out_channels=(in_channels + (3 * growth_rate)) * 2,\n","      kernel_size=(1, 1),\n","      padding=0,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.conv3_1 = nn.Conv2d(\n","      in_channels=(in_channels + (3 * growth_rate)) * 2,\n","      out_channels=growth_rate,\n","      kernel_size=(3, 3),\n","      padding=1,\n","      stride=(1, 1),\n","      device=device,\n","    )\n","    self.batch_norm3 = nn.BatchNorm2d((in_channels + (4 * growth_rate)), device=device)\n","\n","    trans_kernel_size = int(1/transition_layer_theta)\n","    self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","    self.activation_func = nn.ReLU()\n","\n","  def forward(self, x):\n","    self.conv0_0_out = self.conv0_0(x)\n","    self.conv0_1_out = self.conv0_1(self.conv0_0_out)\n","    self.concat0_out = torch.concat((self.conv0_1_out, x), dim=1)\n","    self.batch_norm0_out = self.batch_norm0(self.concat0_out)\n","    self.activation0_out = self.activation_func(self.batch_norm0_out)\n","\n","    self.conv1_0_out = self.conv1_0(self.activation0_out)\n","    self.conv1_1_out = self.conv1_1(self.conv1_0_out)\n","    self.concat1_out = torch.concat((self.conv1_1_out, self.activation0_out), dim=1)\n","    self.batch_norm1_out = self.batch_norm1(self.concat1_out)\n","    self.activation1_out = self.activation_func(self.batch_norm1_out)\n","\n","    self.conv2_0_out = self.conv2_0(self.activation1_out)\n","    self.conv2_1_out = self.conv2_1(self.conv2_0_out)\n","    self.concat2_out = torch.concat((self.conv2_1_out, self.activation1_out), dim=1)\n","    self.batch_norm2_out = self.batch_norm2(self.concat2_out)\n","    self.activation2_out = self.activation_func(self.batch_norm2_out)\n","\n","    self.conv3_0_out = self.conv3_0(self.activation2_out)\n","    self.conv3_1_out = self.conv3_1(self.conv3_0_out)\n","    self.concat3_out = torch.concat((self.conv3_1_out, self.activation2_out), dim=1)\n","    self.batch_norm3_out = self.batch_norm3(self.concat3_out)\n","    self.activation3_out = self.activation_func(self.batch_norm3_out)\n","\n","    self.trans_layer_out = self.trans_layer(self.activation3_out)\n","\n","    return self.trans_layer_out\n","\n","# class DenseBlock(nn.Module):\n","#   def __init__(\n","#     self,\n","#     n_conv: int,\n","#     in_channels: int,\n","#     out_channels: int,\n","#     growth_rate: int,\n","#     transition_layer_theta: float,\n","#     device: torch.device = None,\n","#   ):\n","#     super().__init__()\n","#     self.modules = []\n","#     self.batch_norms = []\n","#     if device is None:\n","#       device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     for idx in range(n_conv):\n","#       in_channels_with_growth = in_channels + (idx * growth_rate)\n","#       out_channels_with_growth = in_channels + ((idx + 1) * growth_rate)\n","#       self.modules.append(\n","#         [\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth,\n","#             out_channels=in_channels_with_growth * 2,\n","#             kernel_size=(1, 1),\n","#             padding=0,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#           nn.Conv2d(\n","#             in_channels=in_channels_with_growth * 2,\n","#             out_channels=growth_rate,\n","#             kernel_size=(3, 3),\n","#             padding=1,\n","#             stride=(1, 1),\n","#             device=device,\n","#           ),\n","#         ]\n","#       )\n","#       self.batch_norms.append(nn.BatchNorm2d(out_channels_with_growth, device=device))\n","#     trans_kernel_size = int(1 / transition_layer_theta)\n","#     self.trans_layer = nn.MaxPool3d(kernel_size=(trans_kernel_size, 1, 1))\n","#     self.activation_func = nn.ReLU()\n","\n","#   def forward(self, x):\n","#     layer_outputs = [x]\n","#     for d_block, batch_norm in zip(self.modules, self.batch_norms):\n","#       for module in d_block:\n","#         x = module(x)\n","#       x = torch.concat((x, layer_outputs[-1]), dim=1)\n","#       x = batch_norm(x)\n","#       x = self.activation_func(x)\n","#       layer_outputs.append(x)\n","\n","#     x = self.trans_layer(x)\n","#     return x\n","\n","\n","class CFFNEnergyFunction(nn.Module):\n","  loss = None\n","\n","  def __init__(self, batch_size: int = 88, m_th: float = 0.5, device=device):\n","    super().__init__()\n","    self.m_th = torch.empty(batch_size, device=device).fill_(m_th)\n","    self.zero_tensor = torch.empty(batch_size, device=device).fill_(0)\n","    self.energy_function = nn.MSELoss(reduction=\"none\")\n","\n","  def forward(self, img0, img1, pairs_indicator):\n","    E_w = torch.mean(self.energy_function(img0, img1), dim=1)\n","    real_pairs = (0.5 * torch.mul(pairs_indicator, torch.pow(E_w, 2)))\n","    fake_pairs = torch.mul(\n","        (1 - pairs_indicator),\n","        torch.max(self.zero_tensor, self.energy_function(self.m_th, E_w))\n","        )\n","    self.loss = torch.mean(torch.add(real_pairs, fake_pairs))\n","\n","    return self.loss\n","\n","  def item(self):\n","    return self.loss.item()\n","\n","\n","class CFFN(nn.Module):\n","  dense_conv1_out = None\n","  dense_conv2_out = None\n","  dense_conv3_out = None\n","  dense_conv4_out = None\n","  conv5_out = None\n","  batch_norm5_out = None\n","  activation5_out = None\n","\n","  def __init__(\n","    self,\n","    input_image_shape: Tuple[int, int],\n","    growth_rate: int = 24,\n","    transition_layer_theta: float = 0.5,\n","    learning_rate: float = 1e-3,\n","    m_th: float = 0.5,  # threshold for contrastive loss\n","    batch_size: int = 88,\n","    device: torch.device = None,\n","  ):\n","    super().__init__()\n","    self.conv0 = nn.Conv2d(in_channels=3, out_channels=48, kernel_size=(7, 7), stride=(4, 4))\n","    self.batch_norm0 = nn.BatchNorm2d(48)\n","    self.activation0 = nn.ReLU()\n","    if device is None:\n","      device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    # self.dense_conv1 = DenseBlock(\n","    #   n_conv=2,\n","    #   in_channels=48,\n","    #   out_channels=48,\n","    #   growth_rate=growth_rate,\n","    #   transition_layer_theta=transition_layer_theta,\n","    #   device=device,\n","    # ).to(device)\n","    self.dense_conv1 = DenseBlock2(\n","      in_channels=48,\n","      out_channels=48,\n","      growth_rate=growth_rate,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    )\n","    self.dense_conv2 = DenseBlock3(\n","      in_channels=48,\n","      out_channels=60,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv3 = DenseBlock4(\n","      in_channels=60,\n","      out_channels=78,\n","      growth_rate=24,\n","      transition_layer_theta=transition_layer_theta,\n","      device=device,\n","    ).to(device)\n","    self.dense_conv4 = DenseBlock2(\n","      in_channels=78,\n","      out_channels=126,\n","      growth_rate=24,\n","      transition_layer_theta=1,\n","      device=device,\n","    ).to(device)\n","    self.conv5 = nn.Conv2d(in_channels=126, out_channels=128, kernel_size=(3, 3))\n","    self.batch_norm5 = nn.BatchNorm2d(128)\n","    self.activation5 = nn.ReLU()\n","    # self.fully_connected: Callable = lambda in_conv_n_channels, conv_shape0, conv_shape1: nn.Sequential(\n","    #   nn.Flatten(),\n","    #   nn.Linear(in_conv_n_channels * conv_shape0 * conv_shape1, 128, device=device),\n","    #   nn.ReLU(),\n","    # )\n","    self.flatten = nn.Flatten()\n","    self.fully_connected3 = nn.Linear(78 * 15 * 15, 128, device=device)\n","    self.fully_connected4 = nn.Linear(126 * 15 * 15, 128, device=device)\n","    self.fully_connected5 = nn.Linear(128 * 13 * 13, 128, device=device)\n","    self.activation_func = nn.ReLU()\n","    self.loss = CFFNEnergyFunction(batch_size=batch_size, m_th=m_th, device=device).to(device)\n","    self.optimizer = torch.optim.Adam(\n","        self.parameters(), lr=learning_rate,\n","    )\n","\n","  def forward(self, x):\n","    x = self.conv0(x)\n","    x = self.batch_norm0(x)\n","    x = self.activation0(x)\n","    self.dense_conv1_out = self.dense_conv1(x)\n","    # print(f\"dense_conv1_out.shape = '{self.dense_conv1_out.shape}'\")\n","    self.dense_conv2_out = self.dense_conv2(self.dense_conv1_out)\n","    # print(f\"dense_conv2_out.shape = '{self.dense_conv2_out.shape}'\")\n","    self.dense_conv3_out = self.dense_conv3(self.dense_conv2_out)\n","    # print(f\"dense_conv3_out.shape = '{self.dense_conv3_out.shape}'\")\n","    self.dense_conv4_out = self.dense_conv4(self.dense_conv3_out)\n","    # print(f\"dense_conv4_out.shape = '{self.dense_conv4_out.shape}'\")\n","    self.conv5_out = self.conv5(self.dense_conv4_out)\n","    # print(f\"conv5_out.shape = '{self.conv5_out.shape}'\")\n","    self.batch_norm5_out = self.batch_norm5(self.conv5_out)\n","    self.activation5_out = self.activation5(self.batch_norm5_out)\n","\n","    # fn5_module = self.fully_connected(*self.activation5_out.shape[1:])\n","    # fn5 = fn5_module(self.activation5_out)\n","    activation5_flattened = self.flatten(self.activation5_out)\n","    fn5_out = self.fully_connected5(activation5_flattened)\n","    fn5 = self.activation_func(fn5_out)\n","\n","    # fn4_module = self.fully_connected(*self.dense_conv4_out.shape[1:])\n","    # fn4 = fn4_module(self.dense_conv4_out)\n","    dense_conv4_flattened = self.flatten(self.dense_conv4_out)\n","    fn4_out = self.fully_connected4(dense_conv4_flattened)\n","    fn4 = self.activation_func(fn4_out)\n","\n","    # fn3_module = self.fully_connected(*self.dense_conv3_out.shape[1:])\n","    # fn3 = fn3_module(self.dense_conv3_out)\n","    dense_conv3_flattened = self.flatten(self.dense_conv3_out)\n","    fn3_out = self.fully_connected3(dense_conv3_flattened)\n","    fn3 = self.activation_func(fn3_out)\n","\n","    x_out = torch.cat((fn5, fn4, fn3), dim=1)\n","\n","    # return output of convolution 5, which will be input to classification network\n","    # x_out is discriminative features output used for the pairwise learning for CFFN network\n","    return self.activation5_out, x_out\n","\n","  def loss_back_grad(self, img0, img1, pairs_indicator, back_grad: bool = True):\n","    criterion = self.loss(img0, img1, pairs_indicator)\n","    if back_grad:\n","      criterion.backward()\n","      self.optimizer.step()"],"metadata":{"id":"9wMVtogPRhft","executionInfo":{"status":"ok","timestamp":1680798506589,"user_tz":240,"elapsed":166,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["cffn = CFFN(\n","    input_image_shape=(64, 64),\n","    growth_rate=config_cffn[\"growth_rate\"],\n","    transition_layer_theta=config_cffn[\"transition_layer_theta\"],\n","    learning_rate=config_cffn[\"lr\"],\n","    m_th=config_cffn[\"m_th\"],\n","    batch_size=config_cffn[\"batch_size\"],\n","    device=config_cffn[\"device\"],\n","    ).to(config_cffn[\"device\"])\n","# cffn_models_output_path = config_cffn[\"model_output_path\"] / \"models\"\n","# cffn_models_output_path.mkdir(exist_ok=True, parents=True)\n","# loss_output_path = config_cffn[\"model_output_path\"] / \"loss\"\n","# loss_output_path.mkdir(exist_ok=True, parents=True)"],"metadata":{"id":"Pqn7Uv02CtJQ","executionInfo":{"status":"ok","timestamp":1680798631175,"user_tz":240,"elapsed":583,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["cffn"],"metadata":{"id":"VJ8pXZbOZ97r","executionInfo":{"status":"ok","timestamp":1680798641853,"user_tz":240,"elapsed":139,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"cb174cf0-5786-4d46-9f69-4c1e4b2b0ee8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CFFN(\n","  (conv0): Conv2d(3, 48, kernel_size=(7, 7), stride=(4, 4))\n","  (batch_norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (activation0): ReLU()\n","  (dense_conv1): DenseBlock2(\n","    (conv0_0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(72, 144, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(144, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv2): DenseBlock3(\n","    (conv0_0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(72, 144, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(144, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2_0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n","    (conv2_1): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv3): DenseBlock4(\n","    (conv0_0): Conv2d(60, 120, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(120, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(84, 168, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(168, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2_0): Conv2d(108, 216, kernel_size=(1, 1), stride=(1, 1))\n","    (conv2_1): Conv2d(216, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm2): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv3_0): Conv2d(132, 264, kernel_size=(1, 1), stride=(1, 1))\n","    (conv3_1): Conv2d(264, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm3): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv4): DenseBlock2(\n","    (conv0_0): Conv2d(78, 156, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(156, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(102, 204, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(204, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (conv5): Conv2d(126, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (batch_norm5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (activation5): ReLU()\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fully_connected3): Linear(in_features=17550, out_features=128, bias=True)\n","  (fully_connected4): Linear(in_features=28350, out_features=128, bias=True)\n","  (fully_connected5): Linear(in_features=21632, out_features=128, bias=True)\n","  (activation_func): ReLU()\n","  (loss): CFFNEnergyFunction(\n","    (energy_function): MSELoss()\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from tqdm import tqdm\n","## CFFN TRAINING\n","train: bool = False\n","test: bool = False\n","if train:\n","  epoch_tqdm = tqdm(total=config_cffn[\"n_epochs\"], desc=\"Epochs\", position=0, initial=1)\n","  train_loss = Loss.init()\n","  val_loss = Loss.init()\n","  loss_val_output_path = loss_output_path / \"validation\"\n","  loss_val_output_path.mkdir(exist_ok=True, parents=True)\n","  print(\"Starting Training Loop...\")\n","  for epoch in range(config_cffn[\"n_epochs\"]):\n","    for batch_idx, (img0, img1, pair_indicator) in enumerate(dataloader_train_cffn):\n","      cffn.optimizer.zero_grad()\n","      img0 = img0.to(config_cffn[\"device\"])\n","      img1 = img1.to(config_cffn[\"device\"])\n","      _, img0_discriminative_features = cffn(img0)\n","      _, img1_discriminative_features = cffn(img1)\n","      pair_indicator = torch.tensor(pair_indicator, device=config_cffn[\"device\"])\n","      cffn.loss_back_grad(\n","          img0_discriminative_features,\n","          img1_discriminative_features,\n","          pair_indicator,\n","          )\n","      train_loss += cffn.loss.item()\n","      \n","      if (batch_idx + 1) % 50 == 0:\n","        print(\" [%d/%d] [%d/%d]\\tLoss: %.8f\" % \n","              (epoch + 1, config_cffn[\"n_epochs\"], batch_idx + 1, len(dataloader_train_cffn), train_loss.current_loss))\n","\n","    cffn.eval()\n","    for batch_idx, (img0, img1, pair_indicator) in enumerate(dataloader_val_cffn):\n","      img0 = img0.to(config_cffn[\"device\"])\n","      img1 = img1.to(config_cffn[\"device\"])\n","      _, img0_discriminative_features = cffn(img0)\n","      _, img1_discriminative_features = cffn(img1)\n","      pair_indicator = torch.tensor(pair_indicator, device=config_cffn[\"device\"])\n","      cffn.loss_back_grad(\n","        img0_discriminative_features,\n","        img1_discriminative_features,\n","        pair_indicator,\n","        back_grad=False,\n","      )\n","      val_loss += cffn.loss.item()\n","\n","      if (batch_idx + 1) % 50 == 0:\n","        print(\" [%d/%d] [%d/%d]\\tLoss: %.8f\" % \n","              (epoch + 1, config_cffn[\"n_epochs\"], batch_idx + 1, len(dataloader_val_cffn), val_loss.current_loss))\n","\n","    cffn.train()\n","\n","    epoch_tqdm.update(1)\n","    train_loss.update_for_epoch()\n","    val_loss.update_for_epoch()\n","    if (epoch + 1) % config_cffn[\"chkp_freq\"] == 0:\n","      torch.save(\n","          {\n","            \"CFFN_state_dict\": cffn.state_dict(),\n","            \"CFFN_optimizer\": cffn.optimizer.state_dict(),      \n","          },\n","          str(cffn_models_output_path / f\"CFFN--{epoch+1}.pth\")\n","      )\n","\n","      loss_epoch_out_path = loss_output_path / f\"epoch-loss--{epoch+1}.png\"\n","      save_loss_plot(train_loss.loss_vals_per_epoch, loss_epoch_out_path)\n","      loss_val_epoch_out_path = loss_val_output_path / f\"epoch-loss-val--{epoch+1}.png\"\n","      save_loss_plot(val_loss.loss_vals_per_epoch, loss_val_epoch_out_path)\n","      loss_batch_out_path = loss_output_path / f\"batch-loss--{epoch+1}.png\"\n","      save_loss_plot(train_loss.loss_vals_per_batch, loss_batch_out_path, xlabel=\"Batches\")\n","elif test:\n","  model_file = get_latest_model(cffn_models_output_path)\n","  print(model_file)\n","  checkpoint = torch.load(str(model_file))\n","  cffn.load_state_dict(checkpoint[\"CFFN_state_dict\"])\n","  cffn.to(device)\n","  cffn.eval()\n","  test_loss = Loss.init()\n","  batch_tqdm = tqdm(total=len(dataloader_train_cffn), position=0)\n","  for batch_idx, (img0, img1, pair_indicator) in enumerate(dataloader_val_cffn):\n","    img0 = img0.to(config_cffn[\"device\"])\n","    img1 = img1.to(config_cffn[\"device\"])\n","    _, img0_discriminative_features = cffn(img0)\n","    _, img1_discriminative_features = cffn(img1)\n","    pair_indicator = torch.tensor(pair_indicator, device=config_cffn[\"device\"])\n","    cffn.loss_back_grad(\n","        img0_discriminative_features,\n","        img1_discriminative_features,\n","        pair_indicator,\n","        back_grad=False,\n","        )\n","    test_loss += cffn.loss.item()\n","    \n","    if batch_idx % 50 == 0:\n","      batch_tqdm.write(\" [%d/%d]\\tLoss: %.8f\" % (batch_idx, len(dataloader_val_cffn), test_loss.current_loss))\n","\n","    batch_tqdm.update(1)\n","else:\n","  print(\"PASSING AS 'train' & 'test' ARE BOTH FALSE.\")"],"metadata":{"id":"_fIjnkQ8K8um","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679684096389,"user_tz":240,"elapsed":29,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"45e4bc33-5275-4019-fa9c-3b5d36410d54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["PASSING AS 'train' & 'test' ARE BOTH FALSE.\n"]}]},{"cell_type":"markdown","source":["**2. Classification Network**\n","\n","\"The classification sub-network consists of a convolution layer with two channels, and a fully connected layer with two neurons.\""],"metadata":{"id":"YS-iJPjLUe4v"}},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","\n","@dataclass\n","class Accuracy:\n","  acc_vals_per_batch: List[float]\n","  acc_vals_per_epoch: List[float]\n","  precision_per_epoch: List[float]\n","  recall_per_epoch: List[float]\n","  f1_score_per_epoch: List[float]\n","  correct_hits: np.ndarray\n","  correct_hits_per_epoch: List[np.ndarray]\n","  incorrect_hits: np.ndarray\n","  incorrect_hits_per_epoch: List[np.ndarray]\n","  output_decisions: int\n","  batch_cnt: int = 0\n","  previous_batch_cnt: int = 0\n","  epoch_cnt: int = 0\n","  onehotencoding: bool = True\n","\n","  @classmethod\n","  def from_output_decisions(cls, output_size: int, onehotencoding: bool = True) -> \"Accuracy\":\n","    if onehotencoding:\n","      inc_hits_shape = (output_size, output_size)\n","    else:\n","      inc_hits_shape = (output_size,)\n","    return cls(\n","        acc_vals_per_batch=[],\n","        acc_vals_per_epoch=[],\n","        precision_per_epoch=[],\n","        recall_per_epoch=[],\n","        f1_score_per_epoch=[],\n","        batch_cnt=0,\n","        previous_batch_cnt=0,\n","        epoch_cnt=0,\n","        correct_hits=np.zeros((output_size,)),\n","        correct_hits_per_epoch=[],\n","        incorrect_hits=np.zeros(inc_hits_shape),\n","        incorrect_hits_per_epoch=[],\n","        output_decisions=output_size,\n","        onehotencoding=onehotencoding,\n","    )\n","\n","  def compare_batch(self, targets: torch.Tensor, outputs: torch.Tensor) -> List[Tuple[int, int]]:\n","    # determine accuracy between a batch of targets and outputs to update accuracy\n","    hit: int = 0\n","    indices = None\n","    if self.onehotencoding:\n","      indices = []\n","      for batch_idx, (target, output) in enumerate(zip(targets, outputs)):\n","          max_idx = int(torch.argmax(output))\n","          if bool(\n","              target[max_idx]\n","          ):  # see if the one hot encoding scheme of our output neuron layer determined the highest probability to be the same as the true target label\n","              hit += 1\n","              self.correct_hits[max_idx] += 1\n","              indices.append((batch_idx, max_idx))\n","          else:\n","              self.incorrect_hits[int(torch.argmax(target)), max_idx] += 1\n","    else:\n","      outputs = torch.argmax(outputs, dim=1)\n","      \n","      zero_targets_idx = torch.where(targets == 0)\n","      zero_equality = torch.eq(outputs[zero_targets_idx], targets[zero_targets_idx])\n","      self.correct_hits[0] += int(sum(zero_equality))\n","      hit = int(sum(zero_equality))\n","      self.incorrect_hits[0] += int(sum(~zero_equality))\n","\n","      ones_targets_idx = torch.where(targets == 1)\n","      ones_equality = torch.eq(outputs[ones_targets_idx], targets[ones_targets_idx])\n","      self.correct_hits[1] += int(sum(ones_equality))\n","      hit += int(sum(ones_equality))\n","      self.incorrect_hits[1] += int(sum(~ones_equality))\n","\n","    self.acc_vals_per_batch.append(hit / len(targets))\n","    self.batch_cnt += 1\n","\n","    return indices\n","  \n","  @property\n","  def current_accuracy(self) -> float:\n","    return sum(self.acc_vals_per_batch) / self.batch_cnt\n","\n","  def update_for_epoch(self):\n","    # update accuracy for epoch\n","    self.epoch_cnt += 1\n","    self.acc_vals_per_epoch.append(\n","        sum(self.acc_vals_per_batch[self.previous_batch_cnt : self.batch_cnt])\n","        / (self.batch_cnt - self.previous_batch_cnt)\n","    )\n","    if len(self.correct_hits_per_epoch) == 0:\n","      self.correct_hits_per_epoch.append(self.correct_hits.copy())\n","      self.incorrect_hits_per_epoch.append(self.incorrect_hits.copy())\n","    else:\n","      correct_hits_previous_epoch = self.correct_hits_per_epoch[-1].copy()\n","      self.correct_hits_per_epoch.append(self.correct_hits - correct_hits_previous_epoch)\n","      incorrect_hits_previous_epoch = self.incorrect_hits_per_epoch[-1].copy()\n","      self.incorrect_hits_per_epoch.append(self.incorrect_hits - incorrect_hits_previous_epoch)\n","    self.previous_batch_cnt = self.batch_cnt\n","\n","  @property\n","  def confusion_matrix(self) -> np.ndarray:\n","    # get confusion matrix to better visualize incorrect hits vs correct hits\n","    if self.onehotencoding:\n","      return self.incorrect_hits.copy() + np.diag(self.correct_hits)\n","    else:  # assuming binary classification\n","      mat = np.diag(self.correct_hits)\n","      mat[0, 1] = self.cum_false_positive\n","      mat[1, 0] = self.cum_false_negative\n","      return mat\n","\n","  def save_confusion_matrix(self, output_path: Union[str, Path], categories: str):\n","    # save confusion matrix\n","    df = pd.DataFrame(self.confusion_matrix, index=[i for i in categories], columns=[i for i in categories])\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(df, annot=True)\n","    plt.savefig(output_path)\n","    plt.close()\n","\n","  def roc_curve(self, output_path: Union[str, Path]):\n","    tp = []\n","    fp = []\n","    for epoch in range(self.epoch_cnt):\n","      tp.append(self.true_positive(epoch))\n","      fp.append(self.false_positive(epoch))\n","    tp /= max(tp)\n","    fp /= max(fp)\n","    plt.plot(fp, tp)\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.0])\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC curve\")\n","    plt.savefig(output_path)\n","    plt.close()\n","\n","  def cum_stats_to_csv(self, output_path: Union[str, Path]):\n","    df = pd.DataFrame(\n","        columns=[\n","            \"true_positive\",\n","            \"false_positive\",\n","            \"true_negative\",\n","            \"false_negative\",\n","            \"accuracy\",\n","            \"recall\",\n","            \"precision\",\n","            \"f1_score\",\n","        ]\n","    )\n","    for epoch in range(self.epoch_cnt):\n","        tp = self.true_positive(epoch)\n","        fp = self.false_positive(epoch)\n","        tn = self.true_negative(epoch)\n","        fn = self.false_negative(epoch)\n","        accuracy = self.acc_vals_per_epoch[epoch]\n","        recall = self.recall(epoch)\n","        precision = self.precision(epoch)\n","        f1_score = self.f1_score(epoch)\n","        df.loc[epoch] = [tp, fp, tn, fn, accuracy, recall, precision, f1_score]\n","    cum_tp = self.cum_true_positive\n","    cum_fp = self.cum_false_positive\n","    cum_tn = self.cum_true_negative\n","    cum_fn = self.cum_false_negative\n","    cum_accuracy = np.mean(self.acc_vals_per_epoch)\n","    cum_recall = self.cum_recall\n","    cum_precision = self.cum_precision\n","    cum_f1_score = self.cum_f1_score\n","    df.loc[\"cumulative\"] = [cum_tp, cum_fp, cum_tn, cum_fn, cum_accuracy, cum_recall, cum_precision, cum_f1_score]\n","    df.to_csv(output_path)\n","\n","  def true_positive(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"True Positive only defined for Binary Classification\")\n","    return self.correct_hits_per_epoch[epoch][1]  # 1 = true label, 0 = false label\n","\n","  @property\n","  def cum_true_positive(self) -> int:\n","    tp = 0\n","    for epoch in range(self.epoch_cnt):\n","        tp += self.true_positive(epoch)\n","    return tp\n","\n","  def true_negative(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"True Negative only defined for Binary Classification\")\n","    return self.correct_hits_per_epoch[epoch][0]\n","\n","  @property\n","  def cum_true_negative(self) -> int:\n","    tn = 0\n","    for epoch in range(self.epoch_cnt):\n","        tn += self.true_negative(epoch)\n","    return tn\n","\n","  def false_positive(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"False Positive only defined for Binary Classification\")\n","    return self.incorrect_hits_per_epoch[epoch][0]\n","\n","  @property\n","  def cum_false_positive(self) -> int:\n","    fp = 0\n","    for epoch in range(self.epoch_cnt):\n","        fp += self.false_positive(epoch)\n","    return fp\n","\n","  def false_negative(self, epoch: int) -> int:\n","    if len(self.correct_hits) != 2:\n","        raise RuntimeError(f\"False Negative only defined for Binary Classification\")\n","    return self.incorrect_hits_per_epoch[epoch][1]\n","\n","  @property\n","  def cum_false_negative(self) -> int:\n","    fn = 0\n","    for epoch in range(self.epoch_cnt):\n","        fn += self.false_negative(epoch)\n","    return fn\n","\n","  def precision(self, epoch: int) -> float:\n","    return self.true_positive(epoch) / (self.true_positive(epoch) + self.false_positive(epoch))\n","\n","  @property\n","  def cum_precision(self) -> float:\n","    return self.cum_true_positive / (self.cum_true_positive + self.cum_false_positive)\n","\n","  def recall(self, epoch: int) -> float:\n","    return self.true_positive(epoch) / (self.true_positive(epoch) + self.false_negative(epoch))\n","\n","  @property\n","  def cum_recall(self) -> float:\n","    return self.cum_true_positive / (self.cum_true_positive + self.cum_false_negative)\n","\n","  def f1_score(self, epoch: int) -> float:\n","    return 2 * ((self.precision(epoch) * self.recall(epoch)) / (self.precision(epoch) + self.recall(epoch)))\n","\n","  @property\n","  def cum_f1_score(self) -> float:\n","    return 2 * ((self.cum_precision * self.cum_recall) / (self.cum_precision + self.cum_recall))"],"metadata":{"id":"lusyWGl7Tllw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datetime import datetime\n","\n","def training_plot_paths(output_path: Path, epoch: int) -> Tuple[Union[str, Path], ...]:\n","  output_path = Path(output_path)\n","  if not output_path.exists():\n","    output_path.mkdir(exist_ok=True, parents=True)\n","  time_now = datetime.now()\n","  model_pt_name = time_now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n","  Path(output_path / \"loss\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"accuracy\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"confusion\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"models\").mkdir(exist_ok=True, parents=True)\n","  Path(output_path / \"stats\").mkdir(exist_ok=True, parents=True)\n","\n","  confusion_matrix_path = output_path / \"confusion\" / f\"confusion-matrix-{model_pt_name}--{epoch}.png\"\n","  accuracy_plot_path = output_path / \"accuracy\" / f\"accuracy-{model_pt_name}--{epoch}.png\"\n","  roc_curve_plot_path = output_path / \"stats\" / f\"roc-curve--{model_pt_name}--{epoch}.png\"\n","  cum_stats_csv_path = output_path / \"stats\" / f\"cumulative-stats--{model_pt_name}--{epoch}.csv\"\n","  loss_plot_path = output_path / \"loss\" / f\"loss-{model_pt_name}--{epoch}.png\"\n","  model_output_path = output_path / \"models\" / f\"model-{model_pt_name}--{epoch}.pth\"\n","\n","  return (\n","    model_output_path,\n","    confusion_matrix_path,\n","    accuracy_plot_path,\n","    roc_curve_plot_path,\n","    cum_stats_csv_path,\n","    loss_plot_path,\n","  )"],"metadata":{"id":"ObwAcBaypXpD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder\n","\n","def circular_index(idx: int, upper_bound: int) -> int:\n","  if idx < upper_bound:\n","    return idx\n","  return idx - (upper_bound * (idx // upper_bound))\n","\n","class CelebrityDataClassificationNetwork(CelebrityData):\n","  def __init__(\n","      self,\n","      base_path: Path,\n","      transform = None,\n","      seed = None,\n","      gans_to_skip: Optional[List[str]] = None,\n","      unzip_real_imgs: bool = True,\n","      *,\n","      celebrity_data: Optional[CelebrityData] = None,\n","  ):\n","    if celebrity_data is None:\n","      super().__init__(\n","          base_path=base_path,\n","          transform=transform,\n","          seed=seed,\n","          gans_to_skip=gans_to_skip,\n","          unzip_real_imgs=unzip_real_imgs,\n","      )\n","    else:\n","      super().__init__(**celebrity_data.__dict__)\n","    self.to_tensor = transforms.ToTensor()\n","    self.enc = OneHotEncoder()\n","    self.enc.fit([[0], [1]])\n","\n","  def __getitem__(self, index):\n","    img_path, label, gan_selection = self.choose_real_or_fake_image(index)\n","\n","    img = Image.open(img_path).convert('RGB')\n","    if self.transform is not None:\n","      img = self.transform(img)\n","    else:\n","      img = self.to_tensor(img)\n","\n","    return img, label, gan_selection\n","\n","  def choose_real_or_fake_image(self, index) -> Tuple[str, np.ndarray, str]:\n","    if self.rng.standard_normal() > 0:\n","      img = self.real_imgs[circular_index(index // 2, len(self.real_imgs))]  # divide by 2 b/c we define __len__ as all real & fake images\n","      gan_selection = \"real\"\n","      label = [1]\n","    else:\n","      img, gan_selection = self.fake_image_rand_selection(index)\n","      label = [0]\n","\n","    label = np.squeeze(self.enc.transform(np.column_stack(label).reshape(-1, 1)).toarray())\n","    return img, label.astype(np.float32), gan_selection\n","\n","  def fake_image_rand_selection(self, index) -> str:\n","    rand_selection = self.rng.uniform(low=-0.499, high=self.n_fake_gans - 0.501)\n","    gan_selection = self.fake_image_gan_names[int(np.round(rand_selection))]\n","\n","    return self.fake_imgs[gan_selection][index // (self.n_fake_gans * 2)], gan_selection  # mult den by 2 b/c of definition of __len__ being all real & fake images\n","\n","  def names_of_gans(self) -> List[str]:\n","    return list(self.fake_imgs.keys())\n","\n","  def __len__(self):\n","    return len(self.real_imgs) + self.len_of_fake_imgs\n","\n","  @classmethod\n","  def get_training_and_validation_set(\n","    cls,\n","    base_path: Path,\n","    transform = None,\n","    seed = None,\n","    gans_to_skip: Optional[List[str]] = None,\n","    n_fake_imgs_to_extract: int = 40000,\n","    validation_ratio: float = 0.1,\n","    shuffle: bool = True,\n","  ):\n","    celebrity_data = CelebrityData(\n","      base_path=base_path,\n","      transform=transform,\n","      seed=seed,\n","      gans_to_skip=gans_to_skip,\n","      n_fake_imgs_to_extract=n_fake_imgs_to_extract,\n","      validation_ratio=validation_ratio,\n","    )\n","    val_set = celebrity_data.get_validation_set(shuffle)\n","    train_set = celebrity_data.get_training_set(shuffle)\n","\n","    return cls(\n","      base_path=base_path,\n","      celebrity_data=val_set,\n","    ), cls(\n","      base_path=base_path,\n","      celebrity_data=train_set,\n","    )"],"metadata":{"id":"NGvuBcd_rj6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_output_path = Path(\"/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CN/model\")\n","if not model_output_path.exists():\n","  model_output_path.mkdir(exist_ok=True, parents=True)\n","test_output_path = model_output_path / \"test\"\n","if not test_output_path.exists():\n","  test_output_path.mkdir(exist_ok=True, parents=True)\n","\n","config_cn = { 'batch_size'             : 88,\n","              'image_size'             : 64,\n","              'n_channel'              : 3,\n","              'n_epochs'               : 25,\n","              'n_test_epochs'          : 1,\n","              'lr'                     : 1e-3,\n","              'device'                 : device,\n","              'seed'                   : 999,\n","              'model_output_path'      : model_output_path,\n","              'test_output_path'       : test_output_path,\n","              'chkp_freq'              : 1,  # number of epochs to save model out\n","              'n_workers'              : 4,\n","              'gans_to_skip'           : None,\n","              'test_chkp_freq'         : 1,\n","              'val_ratio'              : 0.2,\n","}"],"metadata":{"id":"pitoJ0iJofGo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["celebrity_data_val_cn, celebrity_data_train_cn = CelebrityDataClassificationNetwork.get_training_and_validation_set(\n","  base_path=dataset_base_path,\n","  transform=transforms.Compose(\n","    [\n","      transforms.Resize(int(config_cn[\"image_size\"] * 1.1)),\n","      transforms.CenterCrop(config_cn[\"image_size\"]),\n","      transforms.ToTensor(),\n","      transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n","    ]\n","  ),\n","  seed=config_cn.get(\"seed\"),\n","  gans_to_skip=config_cn.get(\"gans_to_skip\"),\n","  validation_ratio=config_cn.get(\"val_ratio\", 0.1)\n",")\n","\n","dataloader_val_cn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_val_cn,\n","  shuffle=True,\n","  batch_size=config_cn[\"batch_size\"],\n","  num_workers=config_cn[\"n_workers\"],\n","  drop_last=True,\n","  pin_memory=True,\n",")\n","\n","dataloader_train_cn = torch.utils.data.DataLoader(\n","  dataset=celebrity_data_train_cn,\n","  shuffle=True,\n","  batch_size=config_cn[\"batch_size\"],\n","  num_workers=config_cn[\"n_workers\"],\n","  drop_last=True,\n","  pin_memory=True,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJUhhl7d-QGn","executionInfo":{"status":"ok","timestamp":1679698947742,"user_tz":240,"elapsed":80917,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"66af35eb-04bd-461a-f799-4d74bc3a6b41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting imagery for 'WGAN-CP'\n","Extracting imagery for 'PGGAN'\n","Extracting imagery for 'DCGAN'\n","Extracting imagery for 'LSGAN'\n","Extracting imagery for 'CDCGAN'\n","Extracting imagery for 'WGAN-GP'\n","Extracting RealFaces imagery\n"]}]},{"cell_type":"code","source":["class ClassificationNetwork(nn.Module):\n","  conv_layer_out = None\n","  activation_out = None\n","  global_avg_pool_out = None\n","  flatten_out = None\n","  fully_connected_out = None\n","  softmax_out = None\n","\n","  def __init__(self, learning_rate: float = 1e-3):\n","    super().__init__()\n","    self.conv_layer = nn.Conv2d(in_channels=128, out_channels=2, kernel_size=(3, 3))\n","    self.activation = nn.ReLU()\n","    self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n","    self.flatten = nn.Flatten()\n","    self.fully_connected = nn.Linear(2, 2)\n","    self.softmax = nn.Softmax(dim=1)\n","\n","    self.loss = nn.BCELoss()\n","\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  def forward(self, x):\n","    self.conv_layer_out = self.conv_layer(x)\n","    self.activation_out = self.activation(self.conv_layer_out)\n","    self.global_avg_pool_out = self.global_avg_pool(self.activation_out)\n","    self.flatten_out = self.flatten(self.global_avg_pool_out)\n","    self.fully_connected_out = self.fully_connected(self.flatten_out)\n","    self.softmax_out = self.softmax(self.fully_connected_out)\n","\n","    return self.softmax_out"],"metadata":{"id":"rOfO9_JgC3RV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Load pretrained CFFN model and change to eval mode, since the model is already trained\n","cffn_model_file = get_latest_model(cffn_models_output_path, suffix=\".pth\")\n","print(f\"CFFN model file : '{cffn_model_file}'\")\n","checkpoint = torch.load(str(cffn_model_file), map_location=config_cn[\"device\"])\n","cffn.load_state_dict(checkpoint[\"CFFN_state_dict\"])\n","cffn.to(device)\n","cffn.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsU-zKIaFX1t","executionInfo":{"status":"ok","timestamp":1679698963388,"user_tz":240,"elapsed":1346,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"8785c05b-eccb-4bc4-d253-ba1d9f09727f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 15/15 [00:00<00:00, 35848.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["CFFN model file : '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CFFN/model/models/CFFN--15.pth'\n"]},{"output_type":"execute_result","data":{"text/plain":["CFFN(\n","  (conv0): Conv2d(3, 48, kernel_size=(7, 7), stride=(4, 4))\n","  (batch_norm0): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (activation0): ReLU()\n","  (dense_conv1): DenseBlock2(\n","    (conv0_0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(72, 144, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(144, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv2): DenseBlock3(\n","    (conv0_0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(96, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(72, 144, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(144, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2_0): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n","    (conv2_1): Conv2d(192, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv3): DenseBlock4(\n","    (conv0_0): Conv2d(60, 120, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(120, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(84, 168, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(168, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv2_0): Conv2d(108, 216, kernel_size=(1, 1), stride=(1, 1))\n","    (conv2_1): Conv2d(216, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm2): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv3_0): Conv2d(132, 264, kernel_size=(1, 1), stride=(1, 1))\n","    (conv3_1): Conv2d(264, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm3): BatchNorm2d(156, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(2, 1, 1), stride=(2, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (dense_conv4): DenseBlock2(\n","    (conv0_0): Conv2d(78, 156, kernel_size=(1, 1), stride=(1, 1))\n","    (conv0_1): Conv2d(156, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm0): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (conv1_0): Conv2d(102, 204, kernel_size=(1, 1), stride=(1, 1))\n","    (conv1_1): Conv2d(204, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (batch_norm1): BatchNorm2d(126, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (trans_layer): MaxPool3d(kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n","    (activation_func): ReLU()\n","  )\n","  (conv5): Conv2d(126, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (batch_norm5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (activation5): ReLU()\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (fully_connected3): Linear(in_features=17550, out_features=128, bias=True)\n","  (fully_connected4): Linear(in_features=28350, out_features=128, bias=True)\n","  (fully_connected5): Linear(in_features=21632, out_features=128, bias=True)\n","  (activation_func): ReLU()\n","  (loss): CFFNEnergyFunction(\n","    (energy_function): MSELoss()\n","  )\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","cn = ClassificationNetwork(\n","  learning_rate=config_cn[\"lr\"]\n",").to(device)\n","\n","train: bool = False\n","load_pretrained_cn: bool = True\n","if train:\n","  epoch_tqdm = tqdm(total=config_cn[\"n_epochs\"], position=0, initial=1)\n","  train_loss = {\n","      \"total\": Loss.init(),\n","  }\n","  val_loss = {\n","      \"total\": Loss.init(),\n","  }\n","  train_acc = {\n","      \"total\": Accuracy.from_output_decisions(2, onehotencoding=False),\n","  }\n","  val_acc = {\n","      \"total\": Accuracy.from_output_decisions(2, onehotencoding=False),\n","  }\n","  for gan_name in celebrity_data_train_cn.names_of_gans():\n","    train_loss[gan_name] = Loss.init()\n","    val_loss[gan_name] = Loss.init()\n","    train_acc[gan_name] = Accuracy.from_output_decisions(2, onehotencoding=False)\n","    val_acc[gan_name] = Accuracy.from_output_decisions(2, onehotencoding=False)\n","\n","  cn.train()\n","  bce_loss = nn.BCELoss()\n","  print(\"Starting Training Loop for Classification Network...\")\n","  for epoch in range(config_cn[\"n_epochs\"]):\n","    for batch_idx, (img, label, gan_selection) in enumerate(dataloader_train_cn):\n","      # get indices of images that are from the different GANs or real\n","      vals = np.unique(gan_selection)\n","      gan_select_idx = {}\n","      for val in vals:\n","        idx = np.where(np.array(gan_selection) == val)\n","        gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","\n","      # zero classification network gradients\n","      cn.optimizer.zero_grad()\n","      # cast img & label to device we are working on\n","      img = img.to(config_cn[\"device\"])\n","      label = label.to(config_cn[\"device\"])\n","      # get discriminative features from last convolution output of CFFN\n","      img_cffn, img_discriminative_features = cffn(img)\n","      # classify output from CFFN using classification network\n","      out_cn = cn(img_cffn)\n","      # calculate total loss\n","      criterion = bce_loss(out_cn, label)\n","      # calculate gradients\n","      criterion.backward()\n","      # backpropagate gradients\n","      cn.optimizer.step()\n","      # update loss & accuracy for each \n","      train_loss[\"total\"] += criterion.item()\n","      label = torch.argmax(label, dim=1)  # need to unencode one hot encoded labels for accuracy measurements in binary classification task\n","      train_acc[\"total\"].compare_batch(targets=label, outputs=out_cn)\n","\n","      labels_real = label[gan_select_idx[\"real\"]]\n","      out_cn_real = out_cn[gan_select_idx[\"real\"]]\n","      gan_select_idx.pop(\"real\")\n","      for gan_name, idx in gan_select_idx.items():\n","        labels_ = label[idx]\n","        labels_ = torch.concat([labels_, labels_real], dim=0)\n","        out_cn_ = out_cn[idx]\n","        out_cn_ = torch.concat([out_cn_, out_cn_real], dim=0)\n","        # criterion = bce_loss(out_cn_, labels_)\n","        # train_loss[gan_name] += criterion.item()\n","        train_acc[gan_name].compare_batch(targets=labels_, outputs=out_cn_)\n","\n","      if (batch_idx + 1) % 50 == 0:\n","        epoch_write_str = \" [%d/%d]\\tAcc_Total: %.5f\\tLoss_Total: %.8f\"\n","        epoch_write_vars = [batch_idx+1, len(dataloader_train_cn), train_acc[\"total\"].current_accuracy, train_loss[\"total\"].current_loss]\n","        # for key in celebrity_data_cn.names_of_gans():\n","        #   epoch_write_str += f\"\\tLoss_{key}: %.8f\"\n","        #   epoch_write_vars.append(train_loss[key].current_loss)\n","        \n","        epoch_write_vars = tuple(epoch_write_vars)\n","        # epoch_tqdm.write(epoch_write_str % epoch_write_vars)\n","        print(epoch_write_str % epoch_write_vars)\n","    \n","    # validation\n","    cn.eval()\n","    for batch_idx, (img, label, gan_selection) in enumerate(dataloader_val_cn):\n","      vals = np.unique(gan_selection)\n","      gan_select_idx = {}\n","      for val in vals:\n","        idx = np.where(np.array(gan_selection) == val)\n","        gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","\n","      img = img.to(config_cn[\"device\"])\n","      label = label.to(config_cn[\"device\"])\n","      img_cffn, img_discriminative_features = cffn(img)\n","      out_cn = cn(img_cffn)\n","\n","      criterion = bce_loss(out_cn, label)\n","      val_loss[\"total\"] += criterion.item()\n","      label = torch.argmax(label, dim=1)\n","      val_acc[\"total\"].compare_batch(targets=label, outputs=out_cn)\n","\n","      labels_real = label[gan_select_idx[\"real\"]]\n","      out_cn_real = out_cn[gan_select_idx[\"real\"]]\n","      gan_select_idx.pop(\"real\")\n","      for gan_name, idx in gan_select_idx.items():\n","        labels_ = label[idx]\n","        labels_ = torch.concat([labels_, labels_real], dim=0)\n","        out_cn_ = out_cn[idx]\n","        out_cn_ = torch.concat([out_cn_, out_cn_real], dim=0)\n","        val_acc[gan_name].compare_batch(targets=labels_, outputs=out_cn_)\n","\n","      if (batch_idx + 1) % 50 == 0:\n","        epoch_write_str = \" [%d/%d] [%d/%d]\\tVal_Acc_Total: %.5f\\tVal_Loss_Total: %.8f\"\n","        epoch_write_vars = [epoch+1, config_cn[\"n_epochs\"], batch_idx+1, len(dataloader_val_cn), val_acc[\"total\"].current_accuracy, val_loss[\"total\"].current_loss]\n","        epoch_write_vars = tuple(epoch_write_vars)\n","        print(epoch_write_str % epoch_write_vars)\n","\n","    cn.train()\n","\n","    epoch_tqdm.update(1)\n","    train_loss[\"total\"].update_for_epoch()\n","    val_loss[\"total\"].update_for_epoch()\n","    for key in list(train_loss.keys()):\n","      train_acc[key].update_for_epoch()\n","      val_acc[key].update_for_epoch()\n","    if (epoch + 1) % config_cn[\"chkp_freq\"] == 0:\n","      (\n","        model_output_path,\n","        confusion_matrix_path,\n","        accuracy_plot_path,\n","        roc_curve_plot_path,\n","        cum_stats_csv_path,\n","        loss_plot_path,\n","       ) = training_plot_paths(config_cn[\"model_output_path\"], epoch+1)\n","      torch.save(\n","          {\n","            \"CN_state_dict\": cn.state_dict(),\n","            \"CN_optimizer\": cn.optimizer.state_dict(),\n","          },\n","          str(model_output_path),\n","      )\n","      val_loss_plot_path = loss_plot_path.parent / \"validation\" / loss_plot_path.name\n","      val_loss_plot_path.parent.mkdir(exist_ok=True, parents=True)\n","      val_accuracy_plot_path = accuracy_plot_path.parent / \"validation\" / accuracy_plot_path.name\n","      val_accuracy_plot_path.parent.mkdir(exist_ok=True, parents=True)\n","      val_confusion_matrix_path = confusion_matrix_path.parent / \"validation\" / confusion_matrix_path.name\n","      val_confusion_matrix_path.parent.mkdir(exist_ok=True, parents=True)\n","      val_cum_stats_csv_path = cum_stats_csv_path.parent / \"validation\" / cum_stats_csv_path.name\n","      val_cum_stats_csv_path.parent.mkdir(exist_ok=True, parents=True)\n","      for key in list(train_loss.keys()):\n","        loss_plot_path_ = loss_plot_path.parent / key / loss_plot_path.name\n","        loss_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        val_loss_plot_path_ = val_loss_plot_path.parent / key / val_loss_plot_path.name\n","        val_loss_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        \n","        accuracy_plot_path_ = accuracy_plot_path.parent / key / accuracy_plot_path.name\n","        accuracy_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        val_accuracy_plot_path_ = val_accuracy_plot_path.parent / key / val_accuracy_plot_path.name\n","        val_accuracy_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        confusion_matrix_path_ = confusion_matrix_path.parent / key / confusion_matrix_path.name\n","        confusion_matrix_path_.parent.mkdir(exist_ok=True, parents=True)\n","        val_confusion_matrix_path_ = val_confusion_matrix_path.parent / key / val_confusion_matrix_path.name\n","        val_confusion_matrix_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        roc_curve_plot_path_ = roc_curve_plot_path.parent / key / roc_curve_plot_path.name\n","        roc_curve_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        \n","        cum_stats_csv_path_ = cum_stats_csv_path.parent / key / cum_stats_csv_path.name\n","        cum_stats_csv_path_.parent.mkdir(exist_ok=True, parents=True)\n","        val_cum_stats_csv_path_ = val_cum_stats_csv_path.parent / key / val_cum_stats_csv_path.name\n","        val_cum_stats_csv_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        train_loss_ = train_loss[key]\n","        val_loss_ = val_loss[key]\n","        train_acc_ = train_acc[key]\n","        val_acc_ = val_acc[key]\n","        if key in [\"total\"]:\n","          save_loss_plot(train_loss_.loss_vals_per_epoch, loss_plot_path_)\n","          save_loss_plot(val_loss_.loss_vals_per_epoch, val_loss_plot_path_)\n","        save_accuracy_plot(train_acc_.acc_vals_per_epoch, accuracy_plot_path_)\n","        save_accuracy_plot(val_acc_.acc_vals_per_epoch, val_accuracy_plot_path_)\n","        train_acc_.save_confusion_matrix(confusion_matrix_path_, \"01\")\n","        val_acc_.save_confusion_matrix(val_confusion_matrix_path_, \"01\")\n","        # train_acc_.roc_curve(roc_curve_plot_path_)\n","        train_acc_.cum_stats_to_csv(cum_stats_csv_path_)\n","        val_acc_.cum_stats_to_csv(val_cum_stats_csv_path_)\n","elif load_pretrained_cn:\n","  cn_model_base_path = config_cn[\"model_output_path\"] / \"models\"\n","  cn_model_path = get_latest_model(cn_model_base_path)\n","  print(f\"cn_model_path: '{cn_model_path}'\")\n","  \n","  checkpoint = torch.load(str(cn_model_path), map_location=config_cn[\"device\"])\n","  cn.load_state_dict(checkpoint[\"CN_state_dict\"])\n","\n","  epoch_tqdm = tqdm(total=config_cn[\"n_test_epochs\"], position=0, initial=1)\n","  test_loss = {\n","      \"total\": Loss.init(),\n","  }\n","  test_acc = {\n","      \"total\": Accuracy.from_output_decisions(2, onehotencoding=False),\n","  }\n","  for gan_name in celebrity_data_val_cn.names_of_gans():\n","    test_loss[gan_name] = Loss.init()\n","    test_acc[gan_name] = Accuracy.from_output_decisions(2, onehotencoding=False)\n","  \n","  cn.eval()\n","  bce_loss = nn.BCELoss()\n","  print(\"Testing Classification Network + CFFN...\")\n","  for epoch in range(config_cn[\"n_test_epochs\"]):\n","    for batch_idx, (img, label, gan_selection) in enumerate(dataloader_val_cn):\n","      vals = np.unique(gan_selection)\n","      gan_select_idx = {}\n","      for val in vals:\n","        idx = np.where(np.array(gan_selection) == val)\n","        gan_select_idx[val] = torch.Tensor(idx[0]).to(torch.int64)\n","      \n","      img = img.to(config_cn[\"device\"])\n","      label = label.to(config_cn[\"device\"])\n","\n","      img_cffn, img_discriminative_features = cffn(img)\n","      out_cn = cn(img_cffn)\n","      criterion = bce_loss(out_cn, label)\n","      test_loss[\"total\"] += criterion.item()\n","      label = torch.argmax(label, dim=1)\n","      test_acc[\"total\"].compare_batch(targets=label, outputs=out_cn)\n","\n","      labels_real = label[gan_select_idx[\"real\"]]\n","      out_cn_real = out_cn[gan_select_idx[\"real\"]]\n","      gan_select_idx.pop(\"real\")\n","      for gan_name, idx in gan_select_idx.items():\n","        labels_ = label[idx]\n","        labels_ = torch.concat([labels_, labels_real], dim=0)\n","        out_cn_ = out_cn[idx]\n","        out_cn_ = torch.concat([out_cn_, out_cn_real], dim=0)\n","        # criterion = bce_loss(out_cn_, labels_)\n","        # test_loss[gan_name] += criterion.item()\n","        test_acc[gan_name].compare_batch(targets=labels_, outputs=out_cn_)\n","      \n","      if (batch_idx + 1) % 50 == 0:\n","        epoch_write_str = \" [%d/%d]\\tAcc_Total: %.5f\\tLoss_Total: %.8f\"\n","        epoch_write_vars = [batch_idx + 1, len(dataloader_val_cn), test_acc[\"total\"].current_accuracy, test_loss[\"total\"].current_loss]\n","        for key in celebrity_data_val_cn.names_of_gans():\n","          epoch_write_str += f\"\\tAcc_{key}: %.5f\"\n","          epoch_write_vars.append(test_acc[key].current_accuracy)\n","\n","        epoch_write_vars = tuple(epoch_write_vars)\n","        print(epoch_write_str % epoch_write_vars)\n","    \n","    epoch_tqdm.update(1)\n","    for key in list(test_acc.keys()):\n","      # test_loss[key].update_for_epoch()\n","      test_acc[key].update_for_epoch()\n","    if (epoch + 1) % config_cn[\"test_chkp_freq\"] == 0:\n","      (\n","        _,\n","        confusion_matrix_path,\n","        accuracy_plot_path,\n","        roc_curve_plot_path,\n","        cum_stats_csv_path,\n","        loss_plot_path,\n","      ) = training_plot_paths(config_cn[\"test_output_path\"], epoch+1)\n","\n","      for (key, test_loss_), test_acc_ in zip(test_loss.items(), test_acc.values()):\n","        loss_plot_path_ = loss_plot_path.parent / key / loss_plot_path.name\n","        loss_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        accuracy_plot_path_ = accuracy_plot_path.parent / key / accuracy_plot_path.name\n","        accuracy_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        confusion_matrix_path_ = confusion_matrix_path.parent / key / confusion_matrix_path.name\n","        confusion_matrix_path_.parent.mkdir(exist_ok=True, parents=True)\n","        roc_curve_plot_path_ = roc_curve_plot_path.parent / key / roc_curve_plot_path.name\n","        roc_curve_plot_path_.parent.mkdir(exist_ok=True, parents=True)\n","        cum_stats_csv_path_ = cum_stats_csv_path.parent / key / cum_stats_csv_path.name\n","        cum_stats_csv_path_.parent.mkdir(exist_ok=True, parents=True)\n","\n","        if key in [\"real\"]:\n","          save_loss_plot(test_loss_.loss_vals_per_epoch, loss_plot_path_, plot_labels=[\"validation\"])\n","        save_accuracy_plot(test_acc_.acc_vals_per_epoch, accuracy_plot_path_)\n","        test_acc_.save_confusion_matrix(confusion_matrix_path_, \"01\")\n","        test_acc_.roc_curve(roc_curve_plot_path_)\n","        test_acc_.cum_stats_to_csv(cum_stats_csv_path_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z4YkxLn6_dHo","executionInfo":{"status":"ok","timestamp":1679699123771,"user_tz":240,"elapsed":154734,"user":{"displayName":"Christopher Kelton","userId":"01122786583448380586"}},"outputId":"22edb301-fff2-4b85-9b8f-51cd5d393939"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|| 25/25 [00:00<00:00, 73739.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["cn_model_path: '/content/drive/MyDrive/ECE 792 - Advance Topics in Machine Learning/Code/DeepFakeImageDetection/CN/model/models/model-2023-03-24--17-11-42--25.pth'\n"]},{"output_type":"stream","name":"stderr","text":["\r100%|| 1/1 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Testing Classification Network + CFFN...\n"," [50/1005]\tAcc_Total: 0.99045\tLoss_Total: 0.03316165\tAcc_WGAN-CP: 0.98882\tAcc_PGGAN: 0.98905\tAcc_DCGAN: 0.98837\tAcc_LSGAN: 0.98960\tAcc_CDCGAN: 0.98888\tAcc_WGAN-GP: 0.98675\n"," [100/1005]\tAcc_Total: 0.99023\tLoss_Total: 0.03188170\tAcc_WGAN-CP: 0.98871\tAcc_PGGAN: 0.98887\tAcc_DCGAN: 0.98726\tAcc_LSGAN: 0.98925\tAcc_CDCGAN: 0.98824\tAcc_WGAN-GP: 0.98696\n"," [150/1005]\tAcc_Total: 0.99083\tLoss_Total: 0.02968970\tAcc_WGAN-CP: 0.98873\tAcc_PGGAN: 0.98898\tAcc_DCGAN: 0.98741\tAcc_LSGAN: 0.98915\tAcc_CDCGAN: 0.98808\tAcc_WGAN-GP: 0.98756\n"," [200/1005]\tAcc_Total: 0.99091\tLoss_Total: 0.02988152\tAcc_WGAN-CP: 0.98861\tAcc_PGGAN: 0.98874\tAcc_DCGAN: 0.98734\tAcc_LSGAN: 0.98893\tAcc_CDCGAN: 0.98797\tAcc_WGAN-GP: 0.98761\n"," [250/1005]\tAcc_Total: 0.99100\tLoss_Total: 0.02905051\tAcc_WGAN-CP: 0.98929\tAcc_PGGAN: 0.98916\tAcc_DCGAN: 0.98811\tAcc_LSGAN: 0.98947\tAcc_CDCGAN: 0.98843\tAcc_WGAN-GP: 0.98831\n"," [300/1005]\tAcc_Total: 0.99049\tLoss_Total: 0.03037666\tAcc_WGAN-CP: 0.98880\tAcc_PGGAN: 0.98890\tAcc_DCGAN: 0.98775\tAcc_LSGAN: 0.98908\tAcc_CDCGAN: 0.98793\tAcc_WGAN-GP: 0.98798\n"," [350/1005]\tAcc_Total: 0.99065\tLoss_Total: 0.03026504\tAcc_WGAN-CP: 0.98917\tAcc_PGGAN: 0.98914\tAcc_DCGAN: 0.98785\tAcc_LSGAN: 0.98942\tAcc_CDCGAN: 0.98820\tAcc_WGAN-GP: 0.98828\n"," [400/1005]\tAcc_Total: 0.99054\tLoss_Total: 0.03026158\tAcc_WGAN-CP: 0.98900\tAcc_PGGAN: 0.98885\tAcc_DCGAN: 0.98773\tAcc_LSGAN: 0.98920\tAcc_CDCGAN: 0.98801\tAcc_WGAN-GP: 0.98803\n"," [450/1005]\tAcc_Total: 0.99053\tLoss_Total: 0.03019379\tAcc_WGAN-CP: 0.98921\tAcc_PGGAN: 0.98910\tAcc_DCGAN: 0.98753\tAcc_LSGAN: 0.98939\tAcc_CDCGAN: 0.98822\tAcc_WGAN-GP: 0.98828\n"," [500/1005]\tAcc_Total: 0.99066\tLoss_Total: 0.02980261\tAcc_WGAN-CP: 0.98934\tAcc_PGGAN: 0.98933\tAcc_DCGAN: 0.98786\tAcc_LSGAN: 0.98959\tAcc_CDCGAN: 0.98831\tAcc_WGAN-GP: 0.98830\n"," [550/1005]\tAcc_Total: 0.99072\tLoss_Total: 0.02969711\tAcc_WGAN-CP: 0.98939\tAcc_PGGAN: 0.98937\tAcc_DCGAN: 0.98795\tAcc_LSGAN: 0.98969\tAcc_CDCGAN: 0.98844\tAcc_WGAN-GP: 0.98836\n"," [600/1005]\tAcc_Total: 0.99053\tLoss_Total: 0.03149625\tAcc_WGAN-CP: 0.98905\tAcc_PGGAN: 0.98897\tAcc_DCGAN: 0.98760\tAcc_LSGAN: 0.98926\tAcc_CDCGAN: 0.98808\tAcc_WGAN-GP: 0.98799\n"," [650/1005]\tAcc_Total: 0.99056\tLoss_Total: 0.03129962\tAcc_WGAN-CP: 0.98897\tAcc_PGGAN: 0.98885\tAcc_DCGAN: 0.98760\tAcc_LSGAN: 0.98916\tAcc_CDCGAN: 0.98809\tAcc_WGAN-GP: 0.98797\n"," [700/1005]\tAcc_Total: 0.99063\tLoss_Total: 0.03122572\tAcc_WGAN-CP: 0.98903\tAcc_PGGAN: 0.98885\tAcc_DCGAN: 0.98769\tAcc_LSGAN: 0.98926\tAcc_CDCGAN: 0.98817\tAcc_WGAN-GP: 0.98803\n"," [750/1005]\tAcc_Total: 0.99074\tLoss_Total: 0.03094851\tAcc_WGAN-CP: 0.98918\tAcc_PGGAN: 0.98905\tAcc_DCGAN: 0.98788\tAcc_LSGAN: 0.98940\tAcc_CDCGAN: 0.98831\tAcc_WGAN-GP: 0.98820\n"," [800/1005]\tAcc_Total: 0.99099\tLoss_Total: 0.03019064\tAcc_WGAN-CP: 0.98947\tAcc_PGGAN: 0.98940\tAcc_DCGAN: 0.98830\tAcc_LSGAN: 0.98973\tAcc_CDCGAN: 0.98868\tAcc_WGAN-GP: 0.98859\n"," [850/1005]\tAcc_Total: 0.99095\tLoss_Total: 0.03097663\tAcc_WGAN-CP: 0.98932\tAcc_PGGAN: 0.98931\tAcc_DCGAN: 0.98823\tAcc_LSGAN: 0.98963\tAcc_CDCGAN: 0.98862\tAcc_WGAN-GP: 0.98849\n"," [900/1005]\tAcc_Total: 0.99100\tLoss_Total: 0.03085403\tAcc_WGAN-CP: 0.98941\tAcc_PGGAN: 0.98941\tAcc_DCGAN: 0.98821\tAcc_LSGAN: 0.98970\tAcc_CDCGAN: 0.98877\tAcc_WGAN-GP: 0.98851\n"," [950/1005]\tAcc_Total: 0.99120\tLoss_Total: 0.03035896\tAcc_WGAN-CP: 0.98970\tAcc_PGGAN: 0.98972\tAcc_DCGAN: 0.98848\tAcc_LSGAN: 0.99000\tAcc_CDCGAN: 0.98906\tAcc_WGAN-GP: 0.98884\n"," [1000/1005]\tAcc_Total: 0.99114\tLoss_Total: 0.03040530\tAcc_WGAN-CP: 0.98958\tAcc_PGGAN: 0.98964\tAcc_DCGAN: 0.98837\tAcc_LSGAN: 0.98992\tAcc_CDCGAN: 0.98893\tAcc_WGAN-GP: 0.98877\n"]},{"output_type":"stream","name":"stderr","text":["\r2it [02:29, 149.65s/it]              "]}]}]}